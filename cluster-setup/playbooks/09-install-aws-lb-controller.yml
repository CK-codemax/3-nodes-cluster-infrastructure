---
- name: Install AWS Load Balancer Controller
  hosts: master1
  become: true
  gather_facts: true
  vars_files:
    - ../vars/main.yml

  tasks:
    - name: Add eks-charts Helm repository
      shell: |
        helm repo add eks {{ aws_lbc_repository }}
        helm repo update
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: helm_repo_add
      changed_when: "'eks' in helm_repo_add.stdout or 'has been added' in helm_repo_add.stdout"

    - name: Create namespace for AWS Load Balancer Controller
      shell: |
        kubectl create namespace {{ aws_lbc_namespace }} --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: namespace_create
      changed_when: "'created' in namespace_create.stdout or 'configured' in namespace_create.stdout"

    - name: Create Service Account for AWS Load Balancer Controller
      shell: |
        kubectl create serviceaccount {{ aws_lbc_service_account }} \
          --namespace {{ aws_lbc_namespace }} \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: sa_create
      changed_when: "'created' in sa_create.stdout or 'configured' in sa_create.stdout"

    - name: Get Terraform outputs
      shell: |
        cd {{ terraform_dir }} && terraform output -json
      register: terraform_outputs_json
      changed_when: false
      failed_when: false
      delegate_to: localhost

    - name: Parse Terraform outputs
      set_fact:
        terraform_outputs: "{{ terraform_outputs_json.stdout | from_json }}"

    - name: Set Terraform variables from outputs
      set_fact:
        terraform_aws_region: "{{ terraform_outputs.aws_region.value }}"
        terraform_cluster_name: "{{ terraform_outputs.cluster_name.value }}"
        terraform_env_name: "{{ terraform_outputs.environment.value }}"
        terraform_vpc_id: "{{ terraform_outputs.vpc_id.value }}"
        # Cluster name must match the subnet tags: kubernetes.io/cluster/${env}-${cluster_name}
        terraform_cluster_name_tag: "{{ terraform_outputs.environment.value }}-{{ terraform_outputs.cluster_name.value }}"

    - name: Remove IRSA annotation (not needed for self-managed clusters using instance profiles)
      shell: |
        kubectl annotate serviceaccount {{ aws_lbc_service_account }} \
          eks.amazonaws.com/role-arn- \
          --namespace {{ aws_lbc_namespace }} \
          --overwrite
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: sa_annotate_remove
      changed_when: sa_annotate_remove.rc == 0
      failed_when: false

    - name: Create Helm values file with node affinity for AWS Load Balancer Controller
      copy:
        content: |
          nodeSelector:
            aws-workloads: enabled
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: aws-workloads
                    operator: In
                    values:
                    - enabled
        dest: /tmp/aws-lbc-values.yaml
      register: values_file_create

    - name: Display values file location
      debug:
        msg: "Values file created at {{ values_file_create.dest }}"

    - name: Install AWS Load Balancer Controller using Helm
      shell: |
        helm upgrade --install {{ aws_lbc_release_name }} eks/{{ aws_lbc_chart }} \
          --namespace {{ aws_lbc_namespace }} \
          --version {{ aws_lbc_chart_version }} \
          --set clusterName={{ terraform_cluster_name_tag }} \
          --set serviceAccount.create=false \
          --set serviceAccount.name={{ aws_lbc_service_account }} \
          --set region={{ aws_region }} \
          --set vpcId={{ terraform_vpc_id }} \
          --values /tmp/aws-lbc-values.yaml \
          --wait \
          --timeout {{ helm_timeout }}
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: helm_install
      changed_when: "'STATUS: deployed' in helm_install.stdout or 'has been upgraded' in helm_install.stdout"
      async: 900
      poll: 30
      failed_when: helm_install.rc != 0

    - name: Display Helm installation output
      debug:
        var: helm_install.stdout_lines

    - name: Verify AWS Load Balancer Controller pods are running
      shell: |
        kubectl get pods -n {{ aws_lbc_namespace }} -l app.kubernetes.io/name=aws-load-balancer-controller
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: pod_status
      until: >
        pod_status.stdout is defined and
        ('Running' in pod_status.stdout or 'Completed' in pod_status.stdout) and
        '0/2' not in pod_status.stdout and
        '0/1' not in pod_status.stdout
      retries: 30
      delay: 10
      changed_when: false

    - name: Display pod status
      debug:
        var: pod_status.stdout_lines

    - name: Verify AWS Load Balancer Controller is ready
      shell: |
        kubectl get deployment -n {{ aws_lbc_namespace }} aws-load-balancer-controller -o jsonpath='{.status.readyReplicas}'
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: ready_replicas
      until: ready_replicas.stdout == "1" or ready_replicas.stdout == "2"
      retries: 30
      delay: 10
      changed_when: false

    - name: Display AWS Load Balancer Controller deployment status
      shell: |
        kubectl get deployment -n {{ aws_lbc_namespace }} aws-load-balancer-controller
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: deployment_status
      changed_when: false

    - name: Show deployment status
      debug:
        var: deployment_status.stdout_lines

